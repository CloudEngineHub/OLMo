{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-08T07:16:03.939197Z",
     "start_time": "2024-06-08T07:16:03.914855Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "num_embeddings = 64\n",
    "d_model = 128\n",
    "batch_size = 256\n",
    "dtype = torch.float32"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-08T07:16:03.941031Z",
     "start_time": "2024-06-08T07:16:03.921273Z"
    }
   },
   "id": "11fafd32377be7c1"
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "def l2norm(x: torch.Tensor):\n",
    "    with torch.no_grad():\n",
    "        return torch.linalg.vector_norm(x, dim=-1)\n",
    "\n",
    "def init_parameters(m: nn.Module) -> None:\n",
    "    std = 0.02\n",
    "    cutoff = 3 * std\n",
    "    with torch.no_grad():\n",
    "        if hasattr(m, \"weight\") and m.weight is not None:\n",
    "            nn.init.trunc_normal_(m.weight, mean=0.0, std=std, a=-cutoff, b=cutoff)\n",
    "        if hasattr(m, \"bias\") and m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-08T07:16:03.941297Z",
     "start_time": "2024-06-08T07:16:03.927748Z"
    }
   },
   "id": "457e6d5495fb2fcb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 8.318\tnorms: ['0.226', '0.221', '0.316', '0.316', '0.049', '0.050']\n",
      "loss: 8.318\tnorms: ['0.224', '0.224', '0.318', '0.318', '0.050', '0.051']\n",
      "loss: 8.318\tnorms: ['0.226', '0.222', '0.317', '0.317', '0.049', '0.051']\n",
      "loss: 8.317\tnorms: ['0.227', '0.223', '0.318', '0.318', '0.052', '0.052']\n",
      "loss: 8.317\tnorms: ['0.227', '0.226', '0.321', '0.321', '0.052', '0.053']\n",
      "loss: 8.317\tnorms: ['0.231', '0.229', '0.326', '0.326', '0.055', '0.055']\n",
      "loss: 8.317\tnorms: ['0.235', '0.230', '0.329', '0.329', '0.057', '0.057']\n",
      "loss: 8.317\tnorms: ['0.239', '0.235', '0.335', '0.335', '0.060', '0.060']\n",
      "loss: 8.317\tnorms: ['0.241', '0.236', '0.337', '0.337', '0.062', '0.062']\n",
      "loss: 8.317\tnorms: ['0.244', '0.240', '0.342', '0.342', '0.065', '0.066']\n",
      "loss: 8.317\tnorms: ['0.249', '0.246', '0.349', '0.349', '0.069', '0.069']\n",
      "loss: 8.317\tnorms: ['0.253', '0.249', '0.355', '0.355', '0.073', '0.072']\n",
      "loss: 8.317\tnorms: ['0.259', '0.255', '0.363', '0.363', '0.076', '0.076']\n",
      "loss: 8.316\tnorms: ['0.262', '0.259', '0.368', '0.368', '0.080', '0.080']\n",
      "loss: 8.316\tnorms: ['0.271', '0.265', '0.379', '0.379', '0.086', '0.086']\n",
      "loss: 8.316\tnorms: ['0.275', '0.271', '0.384', '0.384', '0.089', '0.090']\n",
      "loss: 8.316\tnorms: ['0.281', '0.277', '0.395', '0.395', '0.096', '0.095']\n",
      "loss: 8.316\tnorms: ['0.286', '0.282', '0.401', '0.401', '0.100', '0.099']\n",
      "loss: 8.316\tnorms: ['0.296', '0.291', '0.416', '0.416', '0.109', '0.107']\n",
      "loss: 8.316\tnorms: ['0.299', '0.296', '0.421', '0.421', '0.112', '0.111']\n",
      "loss: 8.315\tnorms: ['0.307', '0.305', '0.435', '0.435', '0.120', '0.119']\n",
      "loss: 8.315\tnorms: ['0.313', '0.311', '0.442', '0.442', '0.125', '0.124']\n",
      "loss: 8.315\tnorms: ['0.321', '0.317', '0.453', '0.453', '0.133', '0.130']\n",
      "loss: 8.315\tnorms: ['0.328', '0.323', '0.461', '0.461', '0.138', '0.138']\n",
      "loss: 8.315\tnorms: ['0.335', '0.330', '0.469', '0.469', '0.143', '0.143']\n",
      "loss: 8.314\tnorms: ['0.341', '0.339', '0.483', '0.483', '0.153', '0.152']\n",
      "loss: 8.314\tnorms: ['0.350', '0.345', '0.490', '0.490', '0.159', '0.157']\n",
      "loss: 8.314\tnorms: ['0.358', '0.354', '0.503', '0.503', '0.167', '0.166']\n",
      "loss: 8.314\tnorms: ['0.364', '0.360', '0.510', '0.510', '0.174', '0.173']\n",
      "loss: 8.314\tnorms: ['0.373', '0.369', '0.524', '0.524', '0.182', '0.181']\n",
      "loss: 8.314\tnorms: ['0.380', '0.377', '0.534', '0.534', '0.191', '0.190']\n",
      "loss: 8.313\tnorms: ['0.387', '0.387', '0.547', '0.547', '0.201', '0.200']\n",
      "loss: 8.313\tnorms: ['0.395', '0.391', '0.552', '0.552', '0.207', '0.204']\n",
      "loss: 8.313\tnorms: ['0.403', '0.400', '0.569', '0.569', '0.218', '0.217']\n",
      "loss: 8.313\tnorms: ['0.410', '0.407', '0.575', '0.575', '0.225', '0.223']\n",
      "loss: 8.312\tnorms: ['0.420', '0.416', '0.591', '0.591', '0.236', '0.236']\n",
      "loss: 8.312\tnorms: ['0.427', '0.425', '0.603', '0.603', '0.247', '0.245']\n",
      "loss: 8.312\tnorms: ['0.436', '0.433', '0.615', '0.615', '0.257', '0.255']\n",
      "loss: 8.312\tnorms: ['0.445', '0.440', '0.626', '0.626', '0.266', '0.265']\n",
      "loss: 8.311\tnorms: ['0.453', '0.446', '0.636', '0.636', '0.277', '0.274']\n",
      "loss: 8.311\tnorms: ['0.459', '0.456', '0.647', '0.647', '0.287', '0.285']\n",
      "loss: 8.311\tnorms: ['0.469', '0.464', '0.658', '0.658', '0.297', '0.294']\n",
      "loss: 8.310\tnorms: ['0.475', '0.472', '0.669', '0.669', '0.306', '0.306']\n",
      "loss: 8.310\tnorms: ['0.483', '0.479', '0.680', '0.680', '0.319', '0.316']\n",
      "loss: 8.310\tnorms: ['0.495', '0.491', '0.696', '0.696', '0.333', '0.332']\n",
      "loss: 8.309\tnorms: ['0.501', '0.499', '0.707', '0.707', '0.342', '0.342']\n",
      "loss: 8.309\tnorms: ['0.510', '0.505', '0.717', '0.717', '0.355', '0.353']\n",
      "loss: 8.309\tnorms: ['0.518', '0.516', '0.731', '0.731', '0.367', '0.366']\n",
      "loss: 8.308\tnorms: ['0.530', '0.523', '0.746', '0.746', '0.383', '0.381']\n",
      "loss: 8.308\tnorms: ['0.533', '0.532', '0.751', '0.751', '0.389', '0.388']\n",
      "loss: 8.308\tnorms: ['0.541', '0.541', '0.762', '0.762', '0.402', '0.401']\n",
      "loss: 8.307\tnorms: ['0.553', '0.549', '0.781', '0.781', '0.421', '0.419']\n",
      "loss: 8.307\tnorms: ['0.560', '0.555', '0.787', '0.787', '0.428', '0.428']\n",
      "loss: 8.306\tnorms: ['0.572', '0.567', '0.805', '0.805', '0.447', '0.445']\n",
      "loss: 8.306\tnorms: ['0.578', '0.575', '0.816', '0.816', '0.460', '0.458']\n",
      "loss: 8.306\tnorms: ['0.587', '0.582', '0.825', '0.825', '0.473', '0.469']\n",
      "loss: 8.305\tnorms: ['0.595', '0.590', '0.841', '0.841', '0.490', '0.487']\n"
     ]
    }
   ],
   "source": [
    "class CompetingModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.left_emb = nn.Embedding(num_embeddings, d_model, dtype=dtype)\n",
    "        self.right_emb = nn.Embedding(num_embeddings, d_model, dtype=dtype)\n",
    "        self.norm = nn.LayerNorm(d_model, elementwise_affine=False, bias=False, dtype=dtype)\n",
    "        self.left_out = nn.Linear(d_model, num_embeddings, bias=False, dtype=dtype)\n",
    "        self.right_out = nn.Linear(d_model, num_embeddings, bias=False, dtype=dtype)\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        init_parameters(self.left_emb)\n",
    "        init_parameters(self.right_emb)\n",
    "        init_parameters(self.norm)\n",
    "        init_parameters(self.left_out)\n",
    "        init_parameters(self.right_out)\n",
    "        \n",
    "    def forward(self, left_input: torch.Tensor, right_input: torch.Tensor):\n",
    "        l1 = self.left_emb(left_input)\n",
    "        r1 = self.right_emb(right_input)\n",
    "        x1 = l1 + r1\n",
    "        x2 = x1 # self.norm(x1)\n",
    "        l2 = self.left_out(x2)\n",
    "        r2 = self.right_out(x2)\n",
    "        l3 = nn.functional.softmax(l2, dim=-1)\n",
    "        r3 = nn.functional.softmax(r2, dim=-1)\n",
    "        return ((l3, r3), (\n",
    "            l2norm(l1),\n",
    "            l2norm(r1),\n",
    "            l2norm(x1),\n",
    "            l2norm(x2),\n",
    "            l2norm(l2),\n",
    "            l2norm(r2),            \n",
    "        ))\n",
    "    \n",
    "model = CompetingModel()\n",
    "model.reset_parameters()\n",
    "\n",
    "optim = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr = 1e-6\n",
    ")\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "torch.manual_seed(1492)\n",
    "\n",
    "for step in range(1000000):\n",
    "    optim.zero_grad()\n",
    "    left = torch.randint(0, num_embeddings, (batch_size,))\n",
    "    right = torch.randint(0, num_embeddings, (batch_size,))\n",
    "    ys, norms = model(left, right)\n",
    "    left_loss = loss(ys[0], left)\n",
    "    right_loss = loss(ys[1], right)\n",
    "    total_loss = left_loss + right_loss\n",
    "    total_loss.backward()\n",
    "    optim.step()\n",
    "    if step % 1000 == 0:\n",
    "        norm_strings = [f\"{x.mean().item():.3f}\" for x in norms]\n",
    "        print(f\"loss: {total_loss.item():.3f}\\tnorms: {norm_strings}\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-06-08T07:16:03.951410Z"
    }
   },
   "id": "354f4811139791d3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "bc4ae5a0e9b170d9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
