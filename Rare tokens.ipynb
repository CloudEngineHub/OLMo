{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517aed45-8ffb-4203-9f02-0dfdb6d0ab26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from cached_path import cached_path\n",
    "\n",
    "from olmo.config import TrainConfig\n",
    "from olmo.data import build_memmap_dataset, DataCollator, IterableDataset\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "\n",
    "import torch.distributed as dist\n",
    "dist.init_process_group(backend=\"gloo\", world_size=1, rank=0, store=dist.HashStore())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52040f08-d9c1-428e-b95a-e21a50636858",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config_path = \"configs/mitchish7-s3.yaml\"\n",
    "train_config = TrainConfig.load(train_config_path, [\"global_train_batch_size=1024\"])\n",
    "cfg = train_config\n",
    "# Fill some configuration options.\n",
    "cfg.model.precision = cfg.precision\n",
    "cfg.device_train_batch_size = cfg.global_train_batch_size // 1\n",
    "assert cfg.device_train_batch_size is not None  # for mypy\n",
    "cfg.device_train_grad_accum = cfg.device_train_batch_size // cfg.device_train_microbatch_size\n",
    "if cfg.optimizer.no_decay_norm_and_bias is not None:\n",
    "    log.warning(\n",
    "        \"You set the deprecated config option `no_decay_norm_and_bias`. For compatibility, this\"\n",
    "        \"setting will take precedence over all other weight decay configurations. Please change\"\n",
    "        \"your config to use `decay_norm_and_bias` and `decay_embeddings` instead.\"\n",
    "    )\n",
    "    cfg.optimizer.decay_norm_and_bias = not cfg.optimizer.no_decay_norm_and_bias\n",
    "    cfg.optimizer.decay_embeddings = not cfg.optimizer.no_decay_norm_and_bias\n",
    "    cfg.optimizer.no_decay_norm_and_bias = None  # So nobody uses this by accident.\n",
    "\n",
    "collator = DataCollator(\n",
    "    pad_direction=train_config.data.pad_direction,\n",
    "    pad_token_id=train_config.model.pad_token_id\n",
    ")\n",
    "dataset = build_memmap_dataset(train_config, train_config.data, include_instance_metadata=False)\n",
    "seed = train_config.data.seed if train_config.data.seed is not None else train_config.seed\n",
    "work_dir = Path(\"./temp-work-dir\")\n",
    "loader = DataLoader(\n",
    "        IterableDataset(\n",
    "            dataset,  # type: ignore\n",
    "            train_config.global_train_batch_size,\n",
    "            seed=seed + (train_config.epoch or 0),\n",
    "            shuffle=True,\n",
    "            drop_last=train_config.data.drop_last,\n",
    "            work_dir=None,\n",
    "        ),\n",
    "        batch_size=train_config.device_train_batch_size,\n",
    "        drop_last=train_config.data.drop_last,\n",
    "        collate_fn=collator,\n",
    "        num_workers=train_config.data.num_workers,\n",
    "        pin_memory=train_config.data.pin_memory,\n",
    "        prefetch_factor=None if train_config.data.num_workers == 0 else train_config.data.prefetch_factor,\n",
    "        persistent_workers=False if train_config.data.num_workers == 0 else train_config.data.persistent_workers,\n",
    "        timeout=train_config.data.timeout,\n",
    "    )\n",
    "batches = iter(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "44793209-a40d-442b-910b-ad0393f480ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_step = 500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213a2df3-1784-44ba-b52b-5b8ee49ed422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "total_counts = torch.zeros((max_step, cfg.model.vocab_size,), dtype=torch.int64)\n",
    "\n",
    "for step, batch in tqdm(enumerate(batches)):\n",
    "    if step > max_step:\n",
    "        break\n",
    "    uniques, counts = batch['input_ids'].flatten().unique(return_counts=True)\n",
    "    total_counts[step, uniques] += counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
