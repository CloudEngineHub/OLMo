{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fd2ec09-cb87-4ebe-8bd9-d97bb0d8c545",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akshitab/virtuals/olmo/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ladder\n",
    "from olmo.scaling.scaling_laws.utils import validation as validation_names\n",
    "from olmo.scaling.scaling_laws.utils import downstream_bpb as downstream_bpb_names\n",
    "from olmo.scaling.scaling_laws.utils import downstream as downstream_names\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from olmo.scaling.scaling_laws.utils import (\n",
    "    get_coefficients_huber,\n",
    "    chinchilla_flops_fit,\n",
    "    grad_chinchilla_flops_fit,\n",
    "    chinchilla_n_d_fit,\n",
    "    grad_chinchilla_n_d_fit,\n",
    "    FinalConfig,\n",
    "    get_final_data_by_name,\n",
    "    get_data_by_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c145a28-1d9c-4f84-b794-229803b10a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "def get_gflops(run_name: str, length_in_tokens: Optional[int] = None):\n",
    "    run_name, size, length = get_name_size_length(run_name)\n",
    "    length_in_tokens = length_in_tokens or ladder.parse_length(length, ladder.parse_size(size))\n",
    "    flops = ladder.MODEL_GFLOPS[size]\n",
    "    return flops * length_in_tokens / 1e9\n",
    "\n",
    "def get_params(run_name: str):\n",
    "    run_name, size, length = get_name_size_length(run_name)\n",
    "    params = ladder.MODEL_PARAMS[size]\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd64d412-a9b7-46fa-92f1-21f35300a1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import csv\n",
    "\n",
    "def get_all_data_by_name(configs, keys):\n",
    "    data_by_name = defaultdict(lambda: defaultdict(lambda: []))\n",
    "    for name, config in configs.items():\n",
    "        for path in config.paths:\n",
    "            with open(path) as file_ref:\n",
    "                reader = csv.DictReader(file_ref)\n",
    "                rows = [row for row in reader]\n",
    "                values = []\n",
    "                for row in rows:\n",
    "                    y = np.mean([float(row[key]) for key in keys])\n",
    "                    data_by_name[name][path].append(y)\n",
    "    return data_by_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad6b3326-58ff-4ed1-870e-28845a2d3b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def size_length_from_path(path):\n",
    "    #wandb/amberish-rulebased/150M-1xC.csv\n",
    "    name = path.split(\"/\")[-1].strip(\".csv\")\n",
    "    return name.split(\"-\")[:2]\n",
    "\n",
    "def get_dataframe(x_dict, y_dict):\n",
    "    df = pd.DataFrame()\n",
    "    xs = []\n",
    "    ys = []\n",
    "    params = []\n",
    "    sizes = []\n",
    "    lengths = []\n",
    "    modes = []\n",
    "    runs = []\n",
    "    for name, path_dict in x_dict.items():\n",
    "        config = configs[name]\n",
    "        for path in path_dict:\n",
    "            size, length = size_length_from_path(path)\n",
    "            run_name = f\"{size}-{length}\"\n",
    "            x_data = x_dict[name][path]\n",
    "            y_data = y_dict[name][path]\n",
    "            xs += x_data\n",
    "            ys += y_data\n",
    "            params += [config.n for _ in range(len(x_data))]\n",
    "            sizes += [size for _ in range(len(x_data))]\n",
    "            lengths += [length for _ in range(len(x_data))]\n",
    "            modes += [config.mode for _ in range(len(x_data))]\n",
    "            runs += [run_name for _ in range(len(x_data))]\n",
    "    df[\"x\"] = xs\n",
    "    df[\"y\"] = ys\n",
    "    df[\"params\"] = params\n",
    "    df[\"size\"] = sizes\n",
    "    df[\"length\"] = lengths\n",
    "    df[\"mode\"] = modes\n",
    "    df[\"run\"] = runs\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dabd4ab7-2e71-4087-b42d-5ee0a09171e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = {\n",
    "    \"150M\": \"darkred\",\n",
    "    \"300M\": \"darkorange\",\n",
    "    \"530M\": \"gold\",\n",
    "    \"750M\": \"darkgreen\",\n",
    "    \"1B\": \"teal\",\n",
    "    #\"3B\":\n",
    "    #\"7B\":\n",
    "}\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "def fit_and_plot_step1(df, ax, x_label, y_label, title=\"Fitting final score\"):\n",
    "    df = df.dropna()\n",
    "\n",
    "    for label in df[\"size\"].unique():\n",
    "        adf = df[df[\"size\"]==label]\n",
    "        ax.scatter(adf[\"x\"], adf[\"y\"], color=\"white\", edgecolors=adf[\"size\"].apply(lambda x: color_map[x]), s=7.0, label=label)\n",
    "\n",
    "    train_nds = list(df[df[\"mode\"]==\"train\"][[\"params\", \"x\"]].itertuples(index=False, name=None))\n",
    "    train_ys = df[df[\"mode\"]==\"train\"][\"y\"]\n",
    "\n",
    "    # fit the parameters\n",
    "    coefficients = get_coefficients_huber(\n",
    "        train_nds,\n",
    "        train_ys,\n",
    "        chinchilla_n_d_fit,\n",
    "        grad_chinchilla_n_d_fit,\n",
    "        p0=[3.0, 6.0, 0.1, 0.2, 1.0],\n",
    "        bounds=[(0, None), (0, None), (0, None), (0, None), (0, None)],\n",
    "        disp=False,\n",
    "    )\n",
    "    a, b, alpha, beta, E = coefficients\n",
    "    A, B = np.exp(a), np.exp(b)\n",
    "\n",
    "    df[\"predicted_y\"] = df.apply(lambda x: chinchilla_n_d_fit([x.params, x.x], coefficients), axis=1)\n",
    "    \n",
    "    eval_row = df[df[\"mode\"]==\"eval\"].iloc[-1]\n",
    "    x = eval_row[\"x\"]\n",
    "    y = eval_row[\"y\"]\n",
    "    y_pred = eval_row[\"predicted_y\"]\n",
    "    rel_error = (y_pred - y) / y\n",
    "    \n",
    "    ax.scatter(x, y, marker=\"x\", color=\"blue\", label=f\"actual = {y:0.4f}\", s=50)\n",
    "    ax.scatter(x, y, marker=\"^\", color=\"black\", label=f\"predicted = {y_pred:0.4}\", s=50)\n",
    "    ax.annotate(\n",
    "        f\"{eval_row['run']}: {rel_error * 100:+.1f}%\",\n",
    "        (x, y),\n",
    "        textcoords=\"offset points\",\n",
    "        xytext=(30, 5),\n",
    "        ha=\"center\",\n",
    "        fontsize=10,\n",
    "        color=\"brown\",\n",
    "    )\n",
    "\n",
    "    for params in df[\"params\"].unique():\n",
    "        plotted_xs = np.linspace(df[df[\"params\"]==params][\"x\"].max(), df[df[\"params\"]==params][\"x\"].min(), 100)\n",
    "        plotted_ys = [chinchilla_n_d_fit([params, x_val], coefficients) for x_val in plotted_xs]\n",
    "    \n",
    "        ax.plot(\n",
    "            plotted_xs,\n",
    "            plotted_ys,\n",
    "            color=\"black\",\n",
    "            linestyle=\"--\",\n",
    "            linewidth=0.8,\n",
    "        )\n",
    "\n",
    "    ax.text(\n",
    "        x=0.25,\n",
    "        y=0.50,\n",
    "        s=f\"L(n, d) = {A:.2f} / n^{alpha:.2f} + {B:.2f} / d^{beta:.2f} + {E:.2f}\",\n",
    "        fontsize=10,\n",
    "        transform=plt.gca().transAxes,\n",
    "    )\n",
    "    \n",
    "    ax.legend(loc=\"upper right\", ncols=1)\n",
    "    \n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.set_title(title)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6863156-0454-4312-833c-8c333a9172b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only include ce loss and the 6 dolma sets, as these are the sets we can include in the paper\n",
    "ce_columns = [\n",
    "    'eval/c4_en-validation/CrossEntropyLoss',\n",
    "    'eval/dolma_books-validation/CrossEntropyLoss',\n",
    "    'eval/dolma_common-crawl-validation/CrossEntropyLoss',\n",
    "    'eval/dolma_pes2o-validation/CrossEntropyLoss',\n",
    "    'eval/dolma_reddit-validation/CrossEntropyLoss',\n",
    "    'eval/dolma_stack-validation/CrossEntropyLoss',\n",
    "    'eval/dolma_wiki-validation/CrossEntropyLoss',\n",
    "]\n",
    "\n",
    "mmlu_names = [\"mmlu_stem\", \"mmlu_humanities\", \"mmlu_social_sciences\", \"mmlu_other\"]\n",
    "\n",
    "tasks = {\n",
    "    \"HellaSwag-0shot\": {\n",
    "        \"bpb\": [\"eval/downstream_bpb/hellaswag_rc_0shot_bpb_bpb\"],\n",
    "        \"score\": [\"eval/downstream/hellaswag_rc_0shot_len_norm\"],\n",
    "    },\n",
    "    \"MMLU-Var\": {\n",
    "        \"bpb\": [f\"eval/downstream_bpb/{n}_var_bpb_bpb\" for n in mmlu_names],\n",
    "        \"score\": [f\"eval/downstream/{n}_var_len_norm\" for n in mmlu_names],\n",
    "        \"x_label\": \"mmlu_var_bpb\",\n",
    "        \"y_label\": \"mmlu_var_score\",\n",
    "    },\n",
    "    \"HellaSwag-5shot\": {\n",
    "        \"bpb\": [\"eval/downstream_bpb/hellaswag_rc_5shot_bpb_bpb\"],\n",
    "        \"score\": [\"eval/downstream/hellaswag_rc_5shot_len_norm\"],\n",
    "    },\n",
    "    \"ARC-Easy-5shot\": {\n",
    "        \"bpb\": [\"eval/downstream_bpb/arc_easy_rc_5shot_bpb_bpb\"],\n",
    "        \"score\": [\"eval/downstream/arc_easy_rc_5shot_acc\"],\n",
    "    },\n",
    "    \"ARC-Challenge-5shot\": {\n",
    "        \"bpb\": [\"eval/downstream_bpb/arc_challenge_rc_5shot_bpb_bpb\"],\n",
    "        \"score\": [\"eval/downstream/arc_challenge_rc_5shot_len_norm\"],\n",
    "    },\n",
    "    \"PiQA-5shot\": {\n",
    "        \"bpb\": [\"eval/downstream_bpb/piqa_rc_5shot_bpb_bpb\"],\n",
    "        \"score\": [\"eval/downstream/piqa_rc_5shot_len_norm\"],\n",
    "    },\n",
    "    \"Winogrande-5shot\": {\n",
    "        \"bpb\": [\"eval/downstream_bpb/winogrande_rc_5shot_bpb_bpb\"],\n",
    "        \"score\": [\"eval/downstream/winogrande_rc_5shot_acc\"],\n",
    "    },\n",
    "    \"OpenbookQA-5shot\": {\n",
    "        \"bpb\": [\"eval/downstream_bpb/openbookqa_rc_5shot_bpb_bpb\"],\n",
    "        \"score\": [\"eval/downstream/openbookqa_rc_5shot_len_norm\"],\n",
    "    },\n",
    "    \"BoolQ-5shot\": {\n",
    "        \"bpb\": [\"eval/downstream_bpb/boolq_rc_5shot_bpb_bpb\"],\n",
    "        \"score\": [\"eval/downstream/boolq_rc_5shot_acc\"],\n",
    "    },\n",
    "    \"SciQ-0shot\": {\n",
    "        \"bpb\": [\"eval/downstream_bpb/sciq_rc_0shot_bpb_bpb\"],\n",
    "        \"score\": [\"eval/downstream/sciq_rc_0shot_acc\"],\n",
    "    },\n",
    "    \"Copa-0shot\": {\n",
    "        \"bpb\": [\"eval/downstream_bpb/copa_rc_0shot_bpb_bpb\"],\n",
    "        \"score\": [\"eval/downstream/copa_rc_0shot_acc\"],\n",
    "    },\n",
    "    \"CSQA-5shot\": {\n",
    "        \"bpb\": [\"eval/downstream_bpb/csqa_rc_5shot_bpb_bpb\"],\n",
    "        \"score\": [\"eval/downstream/csqa_rc_5shot_len_norm\"],\n",
    "    },\n",
    "    \"SocialIQA-5shot\": {\n",
    "        \"bpb\": [\"eval/downstream_bpb/socialiqa_rc_5shot_bpb_bpb\"],\n",
    "        \"score\": [\"eval/downstream/socialiqa_rc_5shot_len_norm\"],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "259f214a-5c31-440a-9555-d8cb59a5ac4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-off hackery to use the _newline metrics\n",
    "\n",
    "from olmo.scaling.scaling_laws.utils import downstream_newline_bpb, downstream_newline\n",
    "\n",
    "df = pd.read_csv(\"wandb/amberish1-newline/1B-3T.csv\")\n",
    "\n",
    "mapper = {}\n",
    "for key in downstream_newline:\n",
    "    if \"hellaswag\" in key or \"winogrande\" in key:\n",
    "        continue\n",
    "    new_key = f\"eval/downstream/{key}\"\n",
    "    old_key = new_key.replace(\"_newline\", \"\")\n",
    "    assert new_key in df.columns, new_key\n",
    "    assert old_key in df.columns, old_key\n",
    "    mapper[new_key] = old_key\n",
    "\n",
    "for key in downstream_newline_bpb:\n",
    "    if \"hellaswag\" in key or \"winogrande\" in key:\n",
    "        continue\n",
    "    new_key = f\"eval/downstream_bpb/{key}_bpb\"\n",
    "    old_key = new_key.replace(\"_newline\", \"\")\n",
    "    assert new_key in df.columns, new_key\n",
    "    assert old_key in df.columns, old_key\n",
    "    mapper[new_key] = old_key\n",
    "\n",
    "df.drop(list(mapper.values()), axis=1).rename(columns=mapper).to_csv(\"wandb/amberish1-newline/1B-3T-renamed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eecc47ed-e531-4316-afb1-14db1342371d",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "    \"150m\": {\n",
    "        \"paths\": [\n",
    "            \"wandb/amberish-rulebased/150M-1xC.csv\",\n",
    "            \"wandb/amberish-rulebased/150M-2xC.csv\",\n",
    "            \"wandb/amberish-rulebased/150M-5xC.csv\",\n",
    "            \"wandb/amberish-rulebased/150M-10xC.csv\"\n",
    "        ],\n",
    "        \"mode\": \"train\",\n",
    "        \"n\": 151898880,\n",
    "        \"label\": \"150m\",\n",
    "        \"color\": \"darkred\"\n",
    "    },\n",
    "    \"300m\": {\n",
    "        \"paths\": [\n",
    "            \"wandb/amberish-rulebased/300M-1xC.csv\",\n",
    "            \"wandb/amberish-rulebased/300M-2xC.csv\",\n",
    "            \"wandb/amberish-rulebased/300M-5xC.csv\",\n",
    "            \"wandb/amberish-rulebased/300M-10xC.csv\"\n",
    "        ],\n",
    "        \"mode\": \"train\",\n",
    "        \"n\": 319980544,\n",
    "        \"label\": \"300m\",\n",
    "        \"color\": \"darkorange\"\n",
    "    },\n",
    "    \"530m\": {\n",
    "        \"paths\": [\n",
    "            \"wandb/amberish-rulebased/530M-1xC.csv\",\n",
    "            \"wandb/amberish-rulebased/530M-2xC.csv\",\n",
    "            \"wandb/amberish-rulebased/530M-5xC.csv\",\n",
    "            \"wandb/amberish-rulebased/530M-10xC.csv\"\n",
    "        ],\n",
    "        \"mode\": \"train\",\n",
    "        \"n\": 530074944,\n",
    "        \"label\": \"530m\",\n",
    "        \"color\": \"gold\"\n",
    "    },\n",
    "    \"700m\": {\n",
    "        \"paths\": [\n",
    "            \"wandb/amberish-rulebased/750M-1xC.csv\",\n",
    "            \"wandb/amberish-rulebased/750M-2xC.csv\",\n",
    "            \"wandb/amberish-rulebased/750M-5xC.csv\",\n",
    "            \"wandb/amberish-rulebased/750M-10xC.csv\"\n",
    "        ],\n",
    "        \"mode\": \"train\",\n",
    "        \"n\": 681297408,\n",
    "        \"label\": \"750m\",\n",
    "        \"color\": \"darkgreen\"\n",
    "    },\n",
    "    \"1b\": {\n",
    "        \"paths\": [\n",
    "            \"wandb/amberish-rulebased/1B-1xC.csv\",\n",
    "            \"wandb/amberish-rulebased/1B-2xC.csv\",\n",
    "            \"wandb/amberish-rulebased/1B-5xC.csv\",\n",
    "            \"wandb/amberish-rulebased/1B-10xC.csv\"\n",
    "        ],\n",
    "        \"mode\": \"train\",\n",
    "        \"n\": 1176832000,\n",
    "        \"label\": \"1b\",\n",
    "        \"color\": \"teal\"\n",
    "    },\n",
    "    \"1b-final\": {\n",
    "        \"paths\": [\n",
    "            #\"wandb/amberish-rulebased/1B-10xC.csv\"\n",
    "            \"wandb/amberish1-newline/1B-3T-renamed.csv\"\n",
    "        ],\n",
    "        \"mode\": \"eval\",\n",
    "        \"n\": 1176832000,\n",
    "        \"label\": \"1b\",\n",
    "        \"color\": \"teal\"\n",
    "    }\n",
    "}\n",
    "\n",
    "configs = {name: FinalConfig(**config) for name, config in configs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90389b82-7f85-4ccb-b2dc-c53d4951d469",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = len(tasks.keys())\n",
    "fig, axes = plt.subplots(rows, 1, figsize=(20, 5 * rows))\n",
    "\n",
    "for i, (task_name, task) in enumerate(tasks.items()):\n",
    "\n",
    "    tokens = get_all_data_by_name(configs, [\"throughput/total_tokens\"])\n",
    "    bpb_loss = get_all_data_by_name(configs, task['bpb'])\n",
    "\n",
    "    df = get_dataframe(tokens, bpb_loss)\n",
    "    fdf = df.groupby('run').apply(lambda rows: rows.iloc[-1], include_groups=False).reset_index()\n",
    "    fdf = fit_and_plot_step1(\n",
    "        fdf,\n",
    "        axes[i],\n",
    "        x_label=\"tokens\",\n",
    "        y_label=str(task.get(\"x_label\", task['bpb'])),\n",
    "    )\n",
    "    \n",
    "    # df = get_dataframe(tokens, bpb_loss)\n",
    "    # df[\"y\"] = df.groupby('run')['y'].transform(lambda x: x.rolling(window=20).mean())\n",
    "    # fit_and_plot_step1(\n",
    "    #     df,\n",
    "    #     axes[i][1],\n",
    "    #     x_label=str(task.get(\"x_label\", task['bpb'])) + \"_moving_avg\",\n",
    "    #     y_label=str(task.get(\"y_label\", task['score'])) + \"_moving_avg\",\n",
    "    #     title=f\"{task_name} Bpb moving average (w=20) to score\"\n",
    "    # )\n",
    "\n",
    "    # df = get_dataframe(tokens, bpb_loss)\n",
    "    # df[\"y\"] = df.groupby('run')['y'].transform(lambda x: x.ewm(alpha=0.5).mean())\n",
    "    # fdf = df.groupby('run').apply(lambda rows: rows.iloc[-1], include_groups=False).reset_index()\n",
    "    # fit_and_plot_step1(\n",
    "    #     fdf,\n",
    "    #     axes[i][1],\n",
    "    #     x_label=str(task.get(\"x_label\", task['bpb'])) + \"_exp_moving_avg\",\n",
    "    #     y_label=str(task.get(\"y_label\", task['score'])) + \"_exp_moving_avg\",\n",
    "    #     title=f\"{task_name} Bpb exp moving average (alpha=0.5) to score\"\n",
    "    # )\n",
    "\n",
    "    # df = get_dataframe(bpb_loss, downstream_loss)\n",
    "    # df = df.groupby('run').apply(lambda x: x.iloc[-int(np.ceil(0.2*len(x))):], include_groups=False).reset_index()\n",
    "    # fit_and_plot(\n",
    "    #     df,\n",
    "    #     axes[i][3],\n",
    "    #     x_label=str(task.get(\"x_label\", task['bpb'])) + \"_last_n%\",\n",
    "    #     y_label=str(task.get(\"y_label\", task['score'])) + \"_last_n%\",\n",
    "    #     title=f\"{task_name} Bpb last n% points (n=20%) to score\"\n",
    "    # )\n",
    "\n",
    "fig.suptitle('Using upto 1B-10xC to predict 1B-3T', fontsize=12)\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.95)\n",
    "#plt.savefig(f\"{FIGURES}/downstream-upto-1B-5xC.png\", dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
